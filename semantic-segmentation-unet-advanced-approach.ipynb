{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":4950383,"sourceType":"datasetVersion","datasetId":2814383}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras.backend as K\nimport tensorflow_addons as tfa","metadata":{"papermill":{"duration":0.191779,"end_time":"2023-02-04T15:44:15.014295","exception":false,"start_time":"2023-02-04T15:44:14.822516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:20.986080Z","iopub.execute_input":"2023-11-28T18:45:20.986909Z","iopub.status.idle":"2023-11-28T18:45:20.992689Z","shell.execute_reply.started":"2023-11-28T18:45:20.986872Z","shell.execute_reply":"2023-11-28T18:45:20.991756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data Set","metadata":{"papermill":{"duration":0.01037,"end_time":"2023-02-04T15:44:15.057314","exception":false,"start_time":"2023-02-04T15:44:15.046944","status":"completed"},"tags":[]}},{"cell_type":"code","source":"segmentations = pd.read_csv(\"/kaggle/input/airbus-ship-detection/train_ship_segmentations_v2.csv\")\nsegmentations['EncodedPixels'] = segmentations['EncodedPixels'].astype('string')","metadata":{"papermill":{"duration":1.285796,"end_time":"2023-02-04T15:44:16.35342","exception":false,"start_time":"2023-02-04T15:44:15.067624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:21.085273Z","iopub.execute_input":"2023-11-28T18:45:21.086221Z","iopub.status.idle":"2023-11-28T18:45:21.758162Z","shell.execute_reply.started":"2023-11-28T18:45:21.086184Z","shell.execute_reply":"2023-11-28T18:45:21.757077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = '/kaggle/input/airbus-ship-detection/train_v2/'\nTEST_DIR = '/kaggle/input/airbus-ship-detection/test_v2/'","metadata":{"papermill":{"duration":4.666522,"end_time":"2023-02-04T15:51:05.342551","exception":false,"start_time":"2023-02-04T15:51:00.676029","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:10.787128Z","iopub.execute_input":"2023-11-28T18:52:10.787435Z","iopub.status.idle":"2023-11-28T18:52:10.795208Z","shell.execute_reply.started":"2023-11-28T18:52:10.787408Z","shell.execute_reply":"2023-11-28T18:52:10.793944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to retrieve a training image based on its name\ndef get_train_image(name: str):\n    # Construct the path to the image file\n    path = f'/kaggle/input/airbus-ship-detection/train_v2/{name}'\n    # Read and return the image using OpenCV\n    return cv2.imread(path)\n\n# Function to extract features from an image and update the given DataFrame row\ndef extract_features_from_image(row: pd.Series) -> pd.Series:\n    # Initialize an empty image array (all black) with shape (768, 768, 3)\n    image = np.zeros((768, 768, 3))  # This line is currently commented out: get_train_image(row['ImageId'])\n    # Update the row with the image's height and width\n    row['ImageHeight'], row['ImageWidth'], _ = image.shape\n    return row\n\n# Apply the extract_features_from_image function to each row in the 'segmentations' DataFrame\nsegmentations = segmentations.apply(lambda x: extract_features_from_image(x), axis=1)\n","metadata":{"papermill":{"duration":386.05566,"end_time":"2023-02-04T15:50:42.554879","exception":false,"start_time":"2023-02-04T15:44:16.499219","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:21.760319Z","iopub.execute_input":"2023-11-28T18:45:21.760617Z","iopub.status.idle":"2023-11-28T18:52:10.778352Z","shell.execute_reply.started":"2023-11-28T18:45:21.760588Z","shell.execute_reply":"2023-11-28T18:52:10.777395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Variables","metadata":{}},{"cell_type":"code","source":"# Number of images without ships to include in the training dataset\nIMAGES_WITHOUT_SHIPS_NUMBER = 25000\n\n# Number of images reserved for validation\nVALIDATION_LENGTH = 2000\n\n# Number of images reserved for testing\nTEST_LENGTH = 2000\n\n# Calculate the number of images for training based on the total images and validation/test set sizes\nTRAIN_LENGTH = len(images_list) - VALIDATION_LENGTH - TEST_LENGTH\n\n# Batch size for training the model\nBATCH_SIZE = 16\n\n# Buffer size for shuffling the training dataset\nBUFFER_SIZE = 1000\n\n# Shape of the input images (height, width)\nIMG_SHAPE = (256, 256)\n\n# Number of classes in the segmentation task (typically foreground and background)\nNUM_CLASSES = 2\n\nrandom.seed(77)","metadata":{"papermill":{"duration":0.043166,"end_time":"2023-02-04T15:51:42.826054","exception":false,"start_time":"2023-02-04T15:51:42.782888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.445378Z","iopub.execute_input":"2023-11-28T18:52:44.445714Z","iopub.status.idle":"2023-11-28T18:52:44.451196Z","shell.execute_reply.started":"2023-11-28T18:52:44.445665Z","shell.execute_reply":"2023-11-28T18:52:44.450244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"papermill":{"duration":0.033656,"end_time":"2023-02-04T15:51:42.466327","exception":false,"start_time":"2023-02-04T15:51:42.432671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Convert Run Length Encoded (RLE) format to binary mask\ndef rle_to_mask(rle: str, shape=(768, 768)):\n    encoded_pixels = np.array(rle.split(), dtype=int)\n    starts = encoded_pixels[::2] - 1\n    ends = starts + encoded_pixels[1::2]\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T \n\n# Convert binary mask to Run Length Encoded (RLE) format\ndef mask_to_rle(img, shape=(768, 768)) -> str:\n    img = img.astype('float32')\n    img = cv2.resize(img, shape, interpolation=cv2.INTER_AREA)\n    img = np.stack(np.vectorize(lambda x: 0 if x < 0.1 else 1)(img), axis=1)\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n# Convert integer labels to one-hot encoded array\ndef one_hot(a, num_classes):\n    return np.squeeze(np.eye(num_classes)[a])\n\n# Function for making predictions using the trained model\ndef predict(image):\n    image = np.expand_dims(image, axis=0)\n    pred_mask = model.predict(image)[0].argmax(axis=-1)  \n    return pred_mask\n","metadata":{"papermill":{"duration":0.047908,"end_time":"2023-02-04T15:51:42.54775","exception":false,"start_time":"2023-02-04T15:51:42.499842","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.326307Z","iopub.execute_input":"2023-11-28T18:52:44.326614Z","iopub.status.idle":"2023-11-28T18:52:44.338838Z","shell.execute_reply.started":"2023-11-28T18:52:44.326585Z","shell.execute_reply":"2023-11-28T18:52:44.337762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"papermill":{"duration":0.033678,"end_time":"2023-02-04T15:51:42.615747","exception":false,"start_time":"2023-02-04T15:51:42.582069","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/airbus-ship-detection/train_ship_segmentations_v2.csv\") # we will define the same ship segmentations dataset again\ndf['EncodedPixels'] = df['EncodedPixels'].astype('string')\n\n# Delete corrupted images\nCORRUPTED_IMAGES = ['6384c3e78.jpg']\ndf = df.drop(df[df['ImageId'].isin(CORRUPTED_IMAGES)].index)\n\n# Dataframe that contains the segmentation for each ship in the image. \ninstance_segmentation = df\n\n# Dataframe that contains the segmentation of all ships in the image.\nimage_segmentation = df.groupby(by=['ImageId'])['EncodedPixels'].apply(lambda x: np.nan if pd.isna(x).any() else ' '.join(x)).reset_index()","metadata":{"papermill":{"duration":37.021013,"end_time":"2023-02-04T15:51:42.397723","exception":false,"start_time":"2023-02-04T15:51:05.37671","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:10.796523Z","iopub.execute_input":"2023-11-28T18:52:10.796875Z","iopub.status.idle":"2023-11-28T18:52:44.322965Z","shell.execute_reply.started":"2023-11-28T18:52:10.796835Z","shell.execute_reply":"2023-11-28T18:52:44.321859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce the number of images without ships\nimages_without_ships = image_segmentation[image_segmentation['EncodedPixels'].isna()]['ImageId'].values[:IMAGES_WITHOUT_SHIPS_NUMBER]\nimages_with_ships = image_segmentation[image_segmentation['EncodedPixels'].notna()]['ImageId'].values\nimages_list = np.append(images_without_ships, images_with_ships)\n\n# remove corrupted images\nimages_list = np.array(list(filter(lambda x: x not in CORRUPTED_IMAGES, images_list)))","metadata":{"papermill":{"duration":0.099078,"end_time":"2023-02-04T15:51:42.748707","exception":false,"start_time":"2023-02-04T15:51:42.649629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.339996Z","iopub.execute_input":"2023-11-28T18:52:44.340343Z","iopub.status.idle":"2023-11-28T18:52:44.444181Z","shell.execute_reply.started":"2023-11-28T18:52:44.340309Z","shell.execute_reply":"2023-11-28T18:52:44.443332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load training images along with masks and sample weights\ndef load_train_image(tensor) -> tuple:\n    # Extract file path from the Tensor and decode it\n    path = tf.get_static_value(tensor).decode(\"utf-8\")\n\n    # Extract image ID from the path\n    image_id = path.split('/')[-1]\n    \n    # Read the input image using OpenCV\n    input_image = cv2.imread(path)\n    \n    # Resize the image to the specified shape\n    input_image = tf.image.resize(input_image, IMG_SHAPE)\n    \n    # Normalize pixel values to the range [0, 1]\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n\n    # Extract the encoded mask for the corresponding image ID\n    encoded_mask = image_segmentation[image_segmentation['ImageId'] == image_id].iloc[0]['EncodedPixels']\n    \n    # Initialize an empty mask array\n    input_mask = np.zeros(IMG_SHAPE + (1,), dtype=np.int8)\n    \n    # If the encoded mask is not empty, convert it to a binary mask\n    if not pd.isna(encoded_mask):\n        input_mask = rle_to_mask(encoded_mask)\n        input_mask = cv2.resize(input_mask, IMG_SHAPE, interpolation=cv2.INTER_AREA)\n        input_mask = np.expand_dims(input_mask, axis=2)\n    \n    # Convert the binary mask to one-hot encoding\n    one_hot_segmentation_mask = one_hot(input_mask, NUM_CLASSES)\n    \n    # Convert the one-hot encoded mask to a TensorFlow tensor\n    input_mask_tensor = tf.convert_to_tensor(one_hot_segmentation_mask, dtype=tf.float32)\n    \n    # Define class weights for the segmentation classes\n    class_weights = tf.constant([0.0005, 0.9995], tf.float32)\n    \n    # Gather sample weights based on the input mask\n    sample_weights = tf.gather(class_weights, indices=tf.cast(input_mask_tensor, tf.int32), name='cast_sample_weights')\n\n    # Return the input image, input mask tensor, and sample weights\n    return input_image, input_mask_tensor, sample_weights\n\n# Create a TensorFlow dataset for the list of image files\nimages_list = tf.data.Dataset.list_files([f'{TRAIN_DIR}{name}' for name in images_list], shuffle=True)\n\n# Map the load_train_image function to create a dataset of image, mask, and sample weight tuples\ntrain_images = images_list.map(lambda x: tf.py_function(load_train_image, [x], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n\n# Split the dataset into training, validation, and test sets\nvalidation_dataset = train_images.take(VALIDATION_LENGTH)\ntest_dataset = train_images.skip(VALIDATION_LENGTH).take(TEST_LENGTH)\ntrain_dataset = train_images.skip(VALIDATION_LENGTH + TEST_LENGTH)\n\n# Create batches of training, validation, and test datasets\ntrain_batches = (\n    train_dataset\n    .repeat()\n    .batch(BATCH_SIZE))\n\nvalidation_batches = validation_dataset.batch(BATCH_SIZE)\n\ntest_batches = test_dataset.batch(BATCH_SIZE)\n","metadata":{"papermill":{"duration":231.205574,"end_time":"2023-02-04T15:55:34.065265","exception":false,"start_time":"2023-02-04T15:51:42.859691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.452785Z","iopub.execute_input":"2023-11-28T18:52:44.453092Z","iopub.status.idle":"2023-11-28T18:52:45.007247Z","shell.execute_reply.started":"2023-11-28T18:52:44.453063Z","shell.execute_reply":"2023-11-28T18:52:45.006168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet segmentation model","metadata":{"papermill":{"duration":0.033864,"end_time":"2023-02-04T15:55:34.134313","exception":false,"start_time":"2023-02-04T15:55:34.100449","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dice coefficient metric for model evaluation\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    # Calculate the intersection between true and predicted masks\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    \n    # Calculate the union of true and predicted masks\n    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n    \n    # Calculate the Dice coefficient with smoothing\n    dice = (2. * intersection + smooth) / (union + smooth)\n    \n    # Return the mean Dice coefficient across all batches\n    return tf.reduce_mean(dice)\n\n# Dice loss function for model training\ndef dice_loss(y_true, y_pred):\n    # Calculate the Dice coefficient\n    dice_coef_val = dice_coef(y_true, y_pred)\n    \n    # Return the complement of the Dice coefficient as the loss\n    return 1 - dice_coef_val\n","metadata":{"papermill":{"duration":0.995281,"end_time":"2023-02-04T15:55:35.232053","exception":false,"start_time":"2023-02-04T15:55:34.236772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.008630Z","iopub.execute_input":"2023-11-28T18:52:45.009999Z","iopub.status.idle":"2023-11-28T18:52:45.018831Z","shell.execute_reply.started":"2023-11-28T18:52:45.009957Z","shell.execute_reply":"2023-11-28T18:52:45.017638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetModel:\n    def __init__(self, input_shape=(128, 128, 3), num_classes=NUM_CLASSES):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self._model = self._build_model()\n\n    @property\n    def model(self) -> tf.keras.Model:\n        return self._model\n\n    def _conv_block(self, x, filters, size, apply_batch_norm=False, apply_instance_norm=False, apply_dropout=False):\n        initializer = tf.random_normal_initializer(0., 0.02)\n        result = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(filters, size, strides=1, padding='same', use_bias=False, kernel_initializer=initializer),\n            tf.keras.layers.BatchNormalization() if apply_batch_norm else tf.keras.layers.Lambda(lambda x: x),\n            tfa.layers.InstanceNormalization() if apply_instance_norm else tf.keras.layers.Lambda(lambda x: x),\n            tf.keras.layers.Activation(tfa.activations.mish),\n            tf.keras.layers.Dropout(0.55) if apply_dropout else tf.keras.layers.Lambda(lambda x: x),\n        ])\n        return result(x)\n\n    def _upsample_block(self, x, filters, size, apply_dropout=False):\n        initializer = tf.random_normal_initializer(0., 0.02)\n        result = tf.keras.Sequential([\n            tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', use_bias=False, kernel_initializer=initializer),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(0.1) if apply_dropout else tf.keras.layers.Lambda(lambda x: x),\n            tf.keras.layers.Activation(tfa.activations.mish),\n        ])\n        return result(x)\n\n    def _build_model(self):\n        inputs = tf.keras.layers.Input(shape=self.input_shape)\n        x = inputs\n        filters_list = [16, 32, 64]\n\n        # Encoder\n        encoder_outputs = []\n        for i, filters in enumerate(filters_list):\n            x = self._conv_block(x, filters, size=3, apply_batch_norm=True, apply_instance_norm=True)\n            print(f\"Encoder Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=1, apply_batch_norm=True, apply_instance_norm=True)\n            encoder_outputs.append(x)\n            x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n\n        x = self._conv_block(x, filters=128, size=3, apply_batch_norm=True)\n        print(f\"Encoder Block {len(filters_list)+1} Output Shape: {x.shape}\")\n        encoder_outputs.append(x)\n\n        # Decoder\n        x = encoder_outputs[-1]\n        for i, (filters, skip) in enumerate(zip(filters_list[::-1], encoder_outputs[-2::-1])):\n            x = self._upsample_block(x, filters, 3)\n            print(f\"Decoder Upsample Block {i+1} Output Shape: {x.shape}\")\n            x = tf.keras.layers.Concatenate()([x, skip])\n            print(f\"Decoder Concatenate Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=3, apply_batch_norm=True)\n            print(f\"Decoder Conv Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=1, apply_batch_norm=True)\n            print(f\"Decoder Conv Block {i+1} Output Shape: {x.shape}\")\n\n        # Output layer\n        last = self._conv_block(x, filters=self.num_classes, size=1)\n        print(f\"Output Block Output Shape: {last.shape}\")\n        outputs = tf.keras.layers.Activation('softmax')(last)\n\n        return tf.keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T18:52:45.020444Z","iopub.execute_input":"2023-11-28T18:52:45.020900Z","iopub.status.idle":"2023-11-28T18:52:45.046386Z","shell.execute_reply.started":"2023-11-28T18:52:45.020860Z","shell.execute_reply":"2023-11-28T18:52:45.045385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. U-Net Architecture\nThe U-Net architecture is commonly used for image segmentation tasks, where the goal is to classify each pixel in an image into a specific class. It consists of an encoder-decoder structure with skip connections.\n\n### 2. Model Class: `UNetModel`\n- **Initialization**: The class is initialized with parameters like `input_shape` (default is (128, 128, 3)) and `num_classes` (default is `NUM_CLASSES`). It creates an instance variable `_model` by calling the `_build_model` method.\n\n- **Properties**:\n  - `model`: Returns the internal Keras model.\n\n### 3. Model Architecture:\n- **Encoder Blocks**:\n  - The encoder consists of convolutional blocks with increasing filters and downsampling through max-pooling.\n  - Batch normalization, instance normalization, Mish activation, and dropout are applied in each block.\n\n- **Decoder Blocks**:\n  - The decoder consists of upsampling blocks that mirror the encoder.\n  - Concatenation is performed with skip connections from the corresponding encoder block.\n  - Batch normalization, Mish activation, and dropout are applied in each block.\n\n- **Output Layer**:\n  - The final layer is a convolutional block with softmax activation to produce pixel-wise class probabilities.\n\n### 4. Printing Output Shapes:\n- The code includes print statements to display the output shapes after each block, aiding in understanding the network's architecture.\n\n### 5. Custom Layers and Activations:\n- The model uses Mish activation from the `tfa.activations` module.\n- It also uses Instance Normalization from the `tfa.layers` module.\n\n### 6. Dropout and Initialization:\n- Dropout is applied with a rate of 0.55 during encoding and 0.1 during decoding.\n- Convolutional layers use a random normal initializer with a mean of 0 and standard deviation of 0.02.","metadata":{"papermill":{"duration":0.03408,"end_time":"2023-02-04T15:55:34.20256","exception":false,"start_time":"2023-02-04T15:55:34.16848","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Number of training epochs\nEPOCHS = 4\n\n# Calculate steps per epoch based on training length and batch size\nSTEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n\n# Define the RectifiedAdam optimizer with learning rate schedule\noptimizer = tfa.optimizers.RectifiedAdam(\n    learning_rate=0.005,\n    total_steps=EPOCHS * STEPS_PER_EPOCH,\n    warmup_proportion=0.3,\n    min_lr=0.00001,\n)\n\n# Apply Lookahead optimization technique to the optimizer\noptimizer = tfa.optimizers.Lookahead(optimizer)\n\n# Define Categorical Crossentropy loss function\nloss = tf.keras.losses.CategoricalCrossentropy()\n\n# Instantiate the UNetModel and get the underlying model\nmodel = UNetModel(IMG_SHAPE + (3,)).model\n\n# Compile the model with the specified optimizer, loss function, and evaluation metric (dice_coef)\nmodel.compile(optimizer=optimizer, \n              loss=loss,  # Alternatively, use dice_loss as the loss function\n              metrics=[dice_coef],\n)\n\n# Calculate the number of trainable parameters in the model\ntrainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])\n\n# Print the total number of trainable parameters in the model\nprint(f'Trainable params: {trainable_params}')","metadata":{"papermill":{"duration":2.363101,"end_time":"2023-02-04T15:55:37.716543","exception":false,"start_time":"2023-02-04T15:55:35.353442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.047859Z","iopub.execute_input":"2023-11-28T18:52:45.048304Z","iopub.status.idle":"2023-11-28T18:52:45.912569Z","shell.execute_reply.started":"2023-11-28T18:52:45.048267Z","shell.execute_reply":"2023-11-28T18:52:45.911498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the filepath for model checkpoints\ncheckpoint_filepath = 'checkpoints/model-checkpoint'\n\n# Create a ModelCheckpoint callback to save the best model based on validation dice coefficient\nsave_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_dice_coef',  # Monitor validation dice coefficient\n    mode='max',  # Save when the monitored quantity is maximized\n    save_best_only=True  # Save only the best model\n)\n\n# Train the model using the training batches and validate on validation batches\nmodel_history = model.fit(\n    train_batches,  # Training dataset\n    epochs=EPOCHS,  # Number of training epochs\n    steps_per_epoch=STEPS_PER_EPOCH,  # Number of steps per training epoch\n    validation_data=validation_batches,  # Validation dataset\n    callbacks=[save_callback]  # List of callbacks, including ModelCheckpoint\n)\n\n# Load the weights of the best model based on validation dice coefficient\nmodel.load_weights(checkpoint_filepath)","metadata":{"papermill":{"duration":29678.925797,"end_time":"2023-02-05T00:10:16.68194","exception":false,"start_time":"2023-02-04T15:55:37.756143","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.913815Z","iopub.execute_input":"2023-11-28T18:52:45.914105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot loss/metric","metadata":{}},{"cell_type":"code","source":"# Extract training and validation loss from the model history\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\n# Plotting the training and validation loss over epochs\nplt.figure()\n\n# Plot training loss in red\nplt.plot(model_history.epoch, loss, 'r', label='Training loss')\n\n# Plot validation loss in green\nplt.plot(model_history.epoch, val_loss, 'C2', label='Validation loss')\n\n# Set the title and labels for the plot\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\n\n# Add a legend to distinguish between training and validation loss\nplt.legend()\n\n# Display the plot\nplt.show()","metadata":{"papermill":{"duration":3.776699,"end_time":"2023-02-05T00:10:24.326871","exception":false,"start_time":"2023-02-05T00:10:20.550172","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same idea as last cell with dice score\ndice_coef_values = model_history.history['dice_coef']\nval_dice_coef_values = model_history.history['val_dice_coef']\n\nplt.figure()\nplt.plot(model_history.epoch, dice_coef_values, 'm', label='Training Dice Coef')\nplt.plot(model_history.epoch, val_dice_coef_values, 'y', label='Validation Dice Coef')\n\nplt.title('Training and Validation Dice Coefficients')\nplt.xlabel('Epoch')\nplt.ylabel('Dice Coefficient Value')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":4.225831,"end_time":"2023-02-05T00:10:31.898498","exception":false,"start_time":"2023-02-05T00:10:27.672667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results visualization","metadata":{"papermill":{"duration":3.134931,"end_time":"2023-02-05T00:10:59.407806","exception":false,"start_time":"2023-02-05T00:10:56.272875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"f, ax = plt.subplots(5, 3, figsize=(15, 15))\ni = 0\n\n# Iterate over the first 5 samples in the test dataset\nfor image, mask in test_dataset.take(5):\n    mask = mask.numpy().argmax(axis=-1)\n\n    # Plot the original image\n    ax[i, 0].imshow(image)\n    ax[i, 0].set_title('Image')\n\n    # Plot the true mask (ground truth)\n    ax[i, 1].imshow(mask, cmap='gray')\n    ax[i, 1].set_title('True Mask')\n\n    # Predict the mask using the 'predict' function and plot the result\n    pred_mask = predict(image)\n    ax[i, 2].imshow(pred_mask, cmap='gray')\n    ax[i, 2].set_title('Predicted Mask')\n\n    i += 1\n\nplt.show()","metadata":{"papermill":{"duration":48.909569,"end_time":"2023-02-05T00:11:51.590225","exception":false,"start_time":"2023-02-05T00:11:02.680656","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.evaluate(test_batches)\nprint(\"test loss, test dice:\", results)","metadata":{"papermill":{"duration":92.661717,"end_time":"2023-02-05T00:13:28.137311","exception":false,"start_time":"2023-02-05T00:11:55.475594","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_metric = dice_coef \ndice_results = []\n\nfor image, true_mask in test_dataset.take(TEST_LENGTH):\n    true_mask = true_mask.numpy().argmax(axis=-1)\n    pred_mask = predict(image)\n    \n    dice_value = dice_metric(true_mask, pred_mask)\n    dice_results.append(dice_value)\n\nplt.hist(dice_results, bins=15)\nprint(\"Mean Dice Coefficient:\", np.mean(dice_results))\n","metadata":{"papermill":{"duration":192.98482,"end_time":"2023-02-05T00:16:44.859519","exception":false,"start_time":"2023-02-05T00:13:31.874699","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":3.468991,"end_time":"2023-02-05T00:16:51.713059","exception":false,"start_time":"2023-02-05T00:16:48.244068","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the sample submission file\nsubmission = pd.read_csv(\"/kaggle/input/airbus-ship-detection/sample_submission_v2.csv\")\n\n# Function to set model predictions for each row in the submission file\ndef set_model_prediction(row: pd.Series) -> pd.Series:\n    # Read and preprocess the image\n    image = cv2.imread(f'{TEST_DIR}{row[\"ImageId\"]}')\n    image = cv2.resize(image, IMG_SHAPE, interpolation=cv2.INTER_AREA)\n    image = image / 255.0\n    \n    # Generate predictions using the model\n    pred_mask = predict(image)\n    \n    # Convert the predicted mask to run-length encoding\n    row['EncodedPixels'] = mask_to_rle(pred_mask)\n    \n    # If the predicted mask is empty, set the 'EncodedPixels' to NaN\n    if row['EncodedPixels'] == '':\n        row['EncodedPixels'] = np.nan\n    \n    return row\n\n# Apply the set_model_prediction function to each row in the submission DataFrame\nsubmission = submission.apply(lambda x: set_model_prediction(x), axis=1)\n\n# Set the 'ImageId' column as the index\nsubmission = submission.set_index(\"ImageId\")\n\n# Save the submission DataFrame to a CSV file\nsubmission.to_csv(\"submission.csv\")\nsubmission\n","metadata":{"papermill":{"duration":2902.09794,"end_time":"2023-02-05T01:05:16.940706","exception":false,"start_time":"2023-02-05T00:16:54.842766","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}