{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":4950383,"sourceType":"datasetVersion","datasetId":2814383}],"dockerImageVersionId":30262,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras.backend as K\nimport tensorflow_addons as tfa","metadata":{"papermill":{"duration":0.191779,"end_time":"2023-02-04T15:44:15.014295","exception":false,"start_time":"2023-02-04T15:44:14.822516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:20.986080Z","iopub.execute_input":"2023-11-28T18:45:20.986909Z","iopub.status.idle":"2023-11-28T18:45:20.992689Z","shell.execute_reply.started":"2023-11-28T18:45:20.986872Z","shell.execute_reply":"2023-11-28T18:45:20.991756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data Set","metadata":{"papermill":{"duration":0.01037,"end_time":"2023-02-04T15:44:15.057314","exception":false,"start_time":"2023-02-04T15:44:15.046944","status":"completed"},"tags":[]}},{"cell_type":"code","source":"segmentations = pd.read_csv(\"/kaggle/input/airbus-ship-detection/train_ship_segmentations_v2.csv\")\nsegmentations['EncodedPixels'] = segmentations['EncodedPixels'].astype('string')","metadata":{"papermill":{"duration":1.285796,"end_time":"2023-02-04T15:44:16.35342","exception":false,"start_time":"2023-02-04T15:44:15.067624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:21.085273Z","iopub.execute_input":"2023-11-28T18:45:21.086221Z","iopub.status.idle":"2023-11-28T18:45:21.758162Z","shell.execute_reply.started":"2023-11-28T18:45:21.086184Z","shell.execute_reply":"2023-11-28T18:45:21.757077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_image(name: str):\n    path = f'/kaggle/input/airbus-ship-detection/train_v2/{name}'\n    return cv2.imread(path)\n\ndef extract_features_from_image(row: pd.Series) -> pd.Series:\n    image = np.zeros((768, 768, 3))# get_train_image(row['ImageId'])\n    row['ImageHeight'], row['ImageWidth'], _ = image.shape\n    return row\n\nsegmentations = segmentations.apply(lambda x: extract_features_from_image(x), axis=1)","metadata":{"papermill":{"duration":386.05566,"end_time":"2023-02-04T15:50:42.554879","exception":false,"start_time":"2023-02-04T15:44:16.499219","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:45:21.760319Z","iopub.execute_input":"2023-11-28T18:45:21.760617Z","iopub.status.idle":"2023-11-28T18:52:10.778352Z","shell.execute_reply.started":"2023-11-28T18:45:21.760588Z","shell.execute_reply":"2023-11-28T18:52:10.777395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Unet Model","metadata":{"papermill":{"duration":0.033754,"end_time":"2023-02-04T15:51:00.642084","exception":false,"start_time":"2023-02-04T15:51:00.60833","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nRANDOM_SEED = 77\nrandom.seed(RANDOM_SEED)\n\nTRAIN_DIR = '/kaggle/input/airbus-ship-detection/train_v2/'\nTEST_DIR = '/kaggle/input/airbus-ship-detection/test_v2/'","metadata":{"papermill":{"duration":4.666522,"end_time":"2023-02-04T15:51:05.342551","exception":false,"start_time":"2023-02-04T15:51:00.676029","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:10.787128Z","iopub.execute_input":"2023-11-28T18:52:10.787435Z","iopub.status.idle":"2023-11-28T18:52:10.795208Z","shell.execute_reply.started":"2023-11-28T18:52:10.787408Z","shell.execute_reply":"2023-11-28T18:52:10.793944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/airbus-ship-detection/train_ship_segmentations_v2.csv\")\ndf['EncodedPixels'] = df['EncodedPixels'].astype('string')\n\n# Delete corrupted images\nCORRUPTED_IMAGES = ['6384c3e78.jpg']\ndf = df.drop(df[df['ImageId'].isin(CORRUPTED_IMAGES)].index)\n\n# Dataframe that contains the segmentation for each ship in the image. \ninstance_segmentation = df\n\n# Dataframe that contains the segmentation of all ships in the image.\nimage_segmentation = df.groupby(by=['ImageId'])['EncodedPixels'].apply(lambda x: np.nan if pd.isna(x).any() else ' '.join(x)).reset_index()","metadata":{"papermill":{"duration":37.021013,"end_time":"2023-02-04T15:51:42.397723","exception":false,"start_time":"2023-02-04T15:51:05.37671","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:10.796523Z","iopub.execute_input":"2023-11-28T18:52:10.796875Z","iopub.status.idle":"2023-11-28T18:52:44.322965Z","shell.execute_reply.started":"2023-11-28T18:52:10.796835Z","shell.execute_reply":"2023-11-28T18:52:44.321859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"papermill":{"duration":0.033656,"end_time":"2023-02-04T15:51:42.466327","exception":false,"start_time":"2023-02-04T15:51:42.432671","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rle_to_mask(rle: str, shape=(768, 768)):\n    '''\n    :param rle: run length encoded pixels as string formated\n           shape: (height,width) of array to return \n    :return: numpy 2D array, 1 - mask, 0 - background\n    '''\n    encoded_pixels = np.array(rle.split(), dtype=int)\n    starts = encoded_pixels[::2] - 1\n    ends = starts + encoded_pixels[1::2]\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef mask_to_rle(img, shape=(768, 768)) -> str:\n    \"\"\"\n    :param img: numpy 2D array, 1 - mask, 0 - background\n           shape: (height,width) dimensions of the image \n    :return: run length encoded pixels as string formated\n    \"\"\"\n    img = img.astype('float32')\n    img = cv2.resize(img, shape, interpolation=cv2.INTER_AREA)\n    img = np.stack(np.vectorize(lambda x: 0 if x < 0.1 else 1)(img), axis=1)\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"papermill":{"duration":0.047908,"end_time":"2023-02-04T15:51:42.54775","exception":false,"start_time":"2023-02-04T15:51:42.499842","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.326307Z","iopub.execute_input":"2023-11-28T18:52:44.326614Z","iopub.status.idle":"2023-11-28T18:52:44.338838Z","shell.execute_reply.started":"2023-11-28T18:52:44.326585Z","shell.execute_reply":"2023-11-28T18:52:44.337762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"papermill":{"duration":0.033678,"end_time":"2023-02-04T15:51:42.615747","exception":false,"start_time":"2023-02-04T15:51:42.582069","status":"completed"},"tags":[]}},{"cell_type":"code","source":"IMAGES_WITHOUT_SHIPS_NUMBER = 25000\n\n# reduce the number of images without ships\nimages_without_ships = image_segmentation[image_segmentation['EncodedPixels'].isna()]['ImageId'].values[:IMAGES_WITHOUT_SHIPS_NUMBER]\nimages_with_ships = image_segmentation[image_segmentation['EncodedPixels'].notna()]['ImageId'].values\nimages_list = np.append(images_without_ships, images_with_ships)\n\n# remove corrupted images\nimages_list = np.array(list(filter(lambda x: x not in CORRUPTED_IMAGES, images_list)))","metadata":{"papermill":{"duration":0.099078,"end_time":"2023-02-04T15:51:42.748707","exception":false,"start_time":"2023-02-04T15:51:42.649629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.339996Z","iopub.execute_input":"2023-11-28T18:52:44.340343Z","iopub.status.idle":"2023-11-28T18:52:44.444181Z","shell.execute_reply.started":"2023-11-28T18:52:44.340309Z","shell.execute_reply":"2023-11-28T18:52:44.443332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALIDATION_LENGTH = 2000\nTEST_LENGTH = 2000\nTRAIN_LENGTH = len(images_list) - VALIDATION_LENGTH - TEST_LENGTH\nBATCH_SIZE = 16\nBUFFER_SIZE = 1000\nIMG_SHAPE = (256, 256)\nNUM_CLASSES = 2","metadata":{"papermill":{"duration":0.043166,"end_time":"2023-02-04T15:51:42.826054","exception":false,"start_time":"2023-02-04T15:51:42.782888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.445378Z","iopub.execute_input":"2023-11-28T18:52:44.445714Z","iopub.status.idle":"2023-11-28T18:52:44.451196Z","shell.execute_reply.started":"2023-11-28T18:52:44.445665Z","shell.execute_reply":"2023-11-28T18:52:44.450244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot(a, num_classes):\n    return np.squeeze(np.eye(num_classes)[a])\n\ndef load_train_image(tensor) -> tuple:\n    path = tf.get_static_value(tensor).decode(\"utf-8\")\n\n    image_id = path.split('/')[-1]\n    input_image = cv2.imread(path)\n    input_image = tf.image.resize(input_image, IMG_SHAPE)\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n\n    encoded_mask = image_segmentation[image_segmentation['ImageId'] == image_id].iloc[0]['EncodedPixels']\n    input_mask = np.zeros(IMG_SHAPE + (1,), dtype=np.int8)\n    if not pd.isna(encoded_mask):\n        input_mask = rle_to_mask(encoded_mask)\n        input_mask = cv2.resize(input_mask, IMG_SHAPE, interpolation=cv2.INTER_AREA)\n        input_mask = np.expand_dims(input_mask, axis=2)\n    one_hot_segmentation_mask = one_hot(input_mask, NUM_CLASSES)\n    input_mask_tensor = tf.convert_to_tensor(one_hot_segmentation_mask, dtype=tf.float32)\n    \n    class_weights = tf.constant([0.0005, 0.9995], tf.float32)\n    sample_weights = tf.gather(class_weights, indices=tf.cast(input_mask_tensor, tf.int32), name='cast_sample_weights')\n\n    return input_image, input_mask_tensor, sample_weights\n\nimages_list = tf.data.Dataset.list_files([f'{TRAIN_DIR}{name}' for name in images_list], shuffle=True)\ntrain_images = images_list.map(lambda x: tf.py_function(load_train_image, [x], [tf.float32, tf.float32]), num_parallel_calls=tf.data.AUTOTUNE)\n\nvalidation_dataset = train_images.take(VALIDATION_LENGTH)\ntest_dataset = train_images.skip(VALIDATION_LENGTH).take(TEST_LENGTH)\ntrain_dataset = train_images.skip(VALIDATION_LENGTH + TEST_LENGTH)\n\ntrain_batches = (\n    train_dataset\n    .repeat()\n    .batch(BATCH_SIZE))\n\nvalidation_batches = validation_dataset.batch(BATCH_SIZE)\n\ntest_batches = test_dataset.batch(BATCH_SIZE)","metadata":{"papermill":{"duration":231.205574,"end_time":"2023-02-04T15:55:34.065265","exception":false,"start_time":"2023-02-04T15:51:42.859691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:44.452785Z","iopub.execute_input":"2023-11-28T18:52:44.453092Z","iopub.status.idle":"2023-11-28T18:52:45.007247Z","shell.execute_reply.started":"2023-11-28T18:52:44.453063Z","shell.execute_reply":"2023-11-28T18:52:45.006168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet segmentation model","metadata":{"papermill":{"duration":0.033864,"end_time":"2023-02-04T15:55:34.134313","exception":false,"start_time":"2023-02-04T15:55:34.100449","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\nFor better accuracy and fewer model parameters, we use a MobileNetV2 classification model with trained weights as an encoder.\n","metadata":{"papermill":{"duration":0.03408,"end_time":"2023-02-04T15:55:34.20256","exception":false,"start_time":"2023-02-04T15:55:34.16848","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def predict(image):\n    image = np.expand_dims(image, axis=0)\n    pred_mask = model.predict(image)[0].argmax(axis=-1)  \n    return pred_mask\n\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(dice)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)","metadata":{"papermill":{"duration":0.995281,"end_time":"2023-02-04T15:55:35.232053","exception":false,"start_time":"2023-02-04T15:55:34.236772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.008630Z","iopub.execute_input":"2023-11-28T18:52:45.009999Z","iopub.status.idle":"2023-11-28T18:52:45.018831Z","shell.execute_reply.started":"2023-11-28T18:52:45.009957Z","shell.execute_reply":"2023-11-28T18:52:45.017638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetModel:\n    def __init__(self, input_shape=(128, 128, 3), num_classes=NUM_CLASSES):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self._model = self._build_model()\n\n    @property\n    def model(self) -> tf.keras.Model:\n        return self._model\n\n    def _conv_block(self, x, filters, size, apply_batch_norm=False, apply_instance_norm=False, apply_dropout=False):\n        initializer = tf.random_normal_initializer(0., 0.02)\n        result = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(filters, size, strides=1, padding='same', use_bias=False, kernel_initializer=initializer),\n            tf.keras.layers.BatchNormalization() if apply_batch_norm else tf.keras.layers.Lambda(lambda x: x),\n            tfa.layers.InstanceNormalization() if apply_instance_norm else tf.keras.layers.Lambda(lambda x: x),\n            tf.keras.layers.Activation(tfa.activations.mish),\n            tf.keras.layers.Dropout(0.55) if apply_dropout else tf.keras.layers.Lambda(lambda x: x),\n        ])\n        return result(x)\n\n    def _upsample_block(self, x, filters, size, apply_dropout=False):\n        initializer = tf.random_normal_initializer(0., 0.02)\n        result = tf.keras.Sequential([\n            tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same', use_bias=False, kernel_initializer=initializer),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dropout(0.1) if apply_dropout else tf.keras.layers.Lambda(lambda x: x),\n            tf.keras.layers.Activation(tfa.activations.mish),\n        ])\n        return result(x)\n\n    def _build_model(self):\n        inputs = tf.keras.layers.Input(shape=self.input_shape)\n        x = inputs\n        filters_list = [16, 32, 64]\n\n        # Encoder\n        encoder_outputs = []\n        for i, filters in enumerate(filters_list):\n            x = self._conv_block(x, filters, size=3, apply_batch_norm=True, apply_instance_norm=True)\n            print(f\"Encoder Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=1, apply_batch_norm=True, apply_instance_norm=True)\n            encoder_outputs.append(x)\n            x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n\n        x = self._conv_block(x, filters=128, size=3, apply_batch_norm=True)\n        print(f\"Encoder Block {len(filters_list)+1} Output Shape: {x.shape}\")\n        encoder_outputs.append(x)\n\n        # Decoder\n        x = encoder_outputs[-1]\n        for i, (filters, skip) in enumerate(zip(filters_list[::-1], encoder_outputs[-2::-1])):\n            x = self._upsample_block(x, filters, 3)\n            print(f\"Decoder Upsample Block {i+1} Output Shape: {x.shape}\")\n            x = tf.keras.layers.Concatenate()([x, skip])\n            print(f\"Decoder Concatenate Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=3, apply_batch_norm=True)\n            print(f\"Decoder Conv Block {i+1} Output Shape: {x.shape}\")\n            x = self._conv_block(x, filters, size=1, apply_batch_norm=True)\n            print(f\"Decoder Conv Block {i+1} Output Shape: {x.shape}\")\n\n        # Output layer\n        last = self._conv_block(x, filters=self.num_classes, size=1)\n        print(f\"Output Block Output Shape: {last.shape}\")\n        outputs = tf.keras.layers.Activation('softmax')(last)\n\n        return tf.keras.Model(inputs=inputs, outputs=outputs)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T18:52:45.020444Z","iopub.execute_input":"2023-11-28T18:52:45.020900Z","iopub.status.idle":"2023-11-28T18:52:45.046386Z","shell.execute_reply.started":"2023-11-28T18:52:45.020860Z","shell.execute_reply":"2023-11-28T18:52:45.045385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 4\nSTEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n\noptimizer = tfa.optimizers.RectifiedAdam(\n    learning_rate=0.005,\n    total_steps=EPOCHS * STEPS_PER_EPOCH,\n    warmup_proportion=0.3,\n    min_lr=0.00001,\n)\noptimizer = tfa.optimizers.Lookahead(optimizer)\n\nloss = tf.keras.losses.CategoricalCrossentropy()\n\nmodel = UNetModel(IMG_SHAPE + (3,)).model\nmodel.compile(optimizer=optimizer, \n              loss=loss, # we can also use dice_loss\n              metrics=[dice_coef],\n)\n\ntrainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])\nprint(f'Trainable params: {trainable_params}')\n","metadata":{"papermill":{"duration":2.363101,"end_time":"2023-02-04T15:55:37.716543","exception":false,"start_time":"2023-02-04T15:55:35.353442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.047859Z","iopub.execute_input":"2023-11-28T18:52:45.048304Z","iopub.status.idle":"2023-11-28T18:52:45.912569Z","shell.execute_reply.started":"2023-11-28T18:52:45.048267Z","shell.execute_reply":"2023-11-28T18:52:45.911498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_filepath = 'checkpoints/model-checkpoint'\nsave_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_dice_coef',\n    mode='max',\n    save_best_only=True\n)\n\nmodel_history = model.fit(train_batches,\n                          epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_data=validation_batches,\n                          callbacks=[save_callback])\nmodel.load_weights(checkpoint_filepath)\n","metadata":{"papermill":{"duration":29678.925797,"end_time":"2023-02-05T00:10:16.68194","exception":false,"start_time":"2023-02-04T15:55:37.756143","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-28T18:52:45.913815Z","iopub.execute_input":"2023-11-28T18:52:45.914105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nplt.figure()\nplt.plot(model_history.epoch, loss, 'r', label='Training loss')\nplt.plot(model_history.epoch, val_loss, 'C2', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":3.776699,"end_time":"2023-02-05T00:10:24.326871","exception":false,"start_time":"2023-02-05T00:10:20.550172","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_coef_values = model_history.history['dice_coef']\nval_dice_coef_values = model_history.history['val_dice_coef']\n\nplt.figure()\nplt.plot(model_history.epoch, dice_coef_values, 'm', label='Training Dice Coef')\nplt.plot(model_history.epoch, val_dice_coef_values, 'y', label='Validation Dice Coef')\n\nplt.title('Training and Validation Dice Coefficients')\nplt.xlabel('Epoch')\nplt.ylabel('Dice Coefficient Value')\nplt.legend()\nplt.show()","metadata":{"papermill":{"duration":4.225831,"end_time":"2023-02-05T00:10:31.898498","exception":false,"start_time":"2023-02-05T00:10:27.672667","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results visualization","metadata":{"papermill":{"duration":3.134931,"end_time":"2023-02-05T00:10:59.407806","exception":false,"start_time":"2023-02-05T00:10:56.272875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"f,ax = plt.subplots(5, 3, figsize=(15, 15))\ni = 0\nfor image, mask in test_dataset.take(N):\n    mask = mask.numpy().argmax(axis=-1)\n    ax[i, 0].imshow(image)\n    ax[i, 0].set_title('image')\n    ax[i, 1].imshow(mask)\n    ax[i, 1].set_title('true mask')\n\n    pred_mask = predict(image)\n    ax[i, 2].imshow(pred_mask)\n    ax[i, 2].set_title('predicted mask')\n    i += 1\n\nplt.show()","metadata":{"papermill":{"duration":48.909569,"end_time":"2023-02-05T00:11:51.590225","exception":false,"start_time":"2023-02-05T00:11:02.680656","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.evaluate(test_batches)\nprint(\"test loss, test dice:\", results)","metadata":{"papermill":{"duration":92.661717,"end_time":"2023-02-05T00:13:28.137311","exception":false,"start_time":"2023-02-05T00:11:55.475594","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_metric = dice_coef \ndice_results = []\n\nfor image, true_mask in test_dataset.take(TEST_LENGTH):\n    true_mask = true_mask.numpy().argmax(axis=-1)\n    pred_mask = predict(image)\n    \n    dice_value = dice_metric(true_mask, pred_mask)\n    dice_results.append(dice_value)\n\nplt.hist(dice_results, bins=15)\nprint(\"Mean Dice Coefficient:\", np.mean(dice_results))\n","metadata":{"papermill":{"duration":192.98482,"end_time":"2023-02-05T00:16:44.859519","exception":false,"start_time":"2023-02-05T00:13:31.874699","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":3.468991,"end_time":"2023-02-05T00:16:51.713059","exception":false,"start_time":"2023-02-05T00:16:48.244068","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/airbus-ship-detection/sample_submission_v2.csv\")\n\ndef set_model_prediction(row: pd.Series) -> pd.Series:\n    image = cv2.imread(f'{TEST_DIR}{row[\"ImageId\"]}')\n    image = cv2.resize(image, IMG_SHAPE, interpolation=cv2.INTER_AREA)\n    image = image / 255.0\n    pred_mask = predict(image)\n    row['EncodedPixels'] = mask_to_rle(pred_mask)\n    if row['EncodedPixels'] == '':\n        row['EncodedPixels'] = np.nan\n    return row\n\nsubmission = submission.apply(lambda x: set_model_prediction(x), axis=1).set_index(\"ImageId\")\n\nsubmission.to_csv(\"./submission.csv\")\nsubmission","metadata":{"papermill":{"duration":2902.09794,"end_time":"2023-02-05T01:05:16.940706","exception":false,"start_time":"2023-02-05T00:16:54.842766","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}